{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"Chapter 24. Advanced Analytics and Machine Learning Overview_nhom9Vietsub.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZyJMbHYAQ5K3"},"source":["# Nhóm 9\n","## Mai Đặng Quân Anh 20170035\n","## Nguyễn Trung Hiếu 20170073\n","## Nguyễn Đình Tuấn Anh 20164767\n","## Lê Ngọc Trường Giang 20170064"]},{"cell_type":"markdown","metadata":{"id":"41n-FUCVQ1bV"},"source":["# What Is MLlib?\n","MLlib là gói được xây dựng trên và là một phần của Spark, có chức năng cung cấp giao diện cho phép tổng hợp và làm sạch dữ liệu, feature engineering and feature selection, huấn luyện và hiệu chỉnh mô hình học máy có giám sát và không giám sát với quy mô lớn, và triển khai các mô hình này.\n","\n","MLlib is a package, built on and included in Spark, that provides interfaces for gathering and\n","cleaning data, feature engineering and feature selection, training and tuning large-scale\n","supervised and unsupervised machine learning models, and using those models in production.\n","## When and why should you use MLlib (versus scikit-learn, TensorFlow, or foo package)\n","Có hai trường hợp chính cần đến khả năng khả mở của Spark. Thứ nhất, Spark có thể rút ngắn thời gian tiền xử lý và sinh thuộc tính trong quá trình tạo bộ dữ liệu cho việc huấn luyện và đánh giá. Sau đó có thể  dùng thư viện ml chỉ hỗ  trợ chạy trên 1 máy với bộ dữ liệu đã được chuẩn bị. Thứ hai, với mô hình hoặc dữ liệu kích thước lớn, Spark giúp việc huấn luyện phân tán trở nên đơn giản.\n","\n","There are two key use cases where you want to leverage Spark’s ability to scale. First, you want\n","to leverage Spark for preprocessing and feature generation to reduce the amount of time it might\n","take to produce training and test sets from a large amount of data. Then you might leverage\n","single-machine learning libraries to train on those given data sets. Second, when your input data\n","or model size become too difficult or inconvenient to put on one machine, use Spark to do the\n","heavy lifting. Spark makes distributed machine learning very simple.\n","# High-Level MLlib Concepts\n","MLlib gồm có các kiểu kiến trúc cơ bản: transformers, estimators, evaluators, và pipelines. Hình 24-2 minh họa quy trình phát triển mô hình học máy trên Spark.\n","\n","In MLlib there are several fundamental “structural” types: transformers, estimators, evaluators,\n","and pipelines. By structural, we mean you will think in terms of these types when you define an\n","end-to-end machine learning pipeline. They’ll provide the common language for defining what\n","belongs in what part of the pipeline. Figure 24-2 illustrates the overall workflow that you will\n","follow when developing machine learning models in Spark.\n","![image.png](attachment:image.png)\n","\n","Transformers là các hàm có chức năng xử lý chuyển đổi dữ liệu. Có thể là: tạo biến mới dựa trên các biến đã có, chuẩn hóa dữ liệu, chuyển đổi kiểu dữ liệu. Transformers thường được sử dụng trong việc tiền xử lý dữ liệu, x\n","ử lý thuộc tính.\n","\n","Transformers are functions that convert raw data in some way. This might be to create a new\n","interaction variable (from two other variables), normalize a column, or simply change an\n","Integer into a Double type to be input into a model. An example of a transformer is one that\n","converts string categorical variables into numerical values that can be used in MLlib.\n","Transformers are primarily used in preprocessing and feature engineering. Transformers take a\n","DataFrame as input and produce a new DataFrame as output, as illustrated in Figure 24-3.\n","![image-2.png](attachment:image-2.png)\n","\n","Estimators có hai loại. Thứ nhất, estimators có thể là transformers mà được khởi tạo bằng dữ liệu. Ví dụ như việc chuẩn hóa dữ liệu, transformers cần được khởi tạo với một số thông tin của dữ liệu sẽ được chuẩn hóa (trung bình, độ lệch chuẩn). Thứ hai, các thuật toán huấn luyện mô hình cũng là Estimators.\n","\n","\n","Estimators are one of two kinds of things. First, estimators can be a kind of transformer that is\n","initialized with data. For instance, to normalize numerical data we’ll need to initialize our\n","transformation with some information about the current values in the column we would like to\n","normalize. This requires two passes over our data—the initial pass generates the initialization\n","values and the second actually applies the generated function over the data. In the Spark’s\n","nomenclature, algorithms that allow users to train a model from data are also referred to as\n","estimators.\n","\n","Evaluator giúp đánh giá mô hình dựa trên tiêu chí được chọn. \n","\n","An evaluator allows us to see how a given model performs according to criteria we specify like a\n","receiver operating characteristic (ROC) curve. After we use an evaluator to select the best model\n","from the ones we tested, we can then use that model to make predictions.\n","From a high level we can specify each of the transformations, estimations, and evaluations one\n","by one, but it is often easier to specify our steps as stages in a pipeline. This pipeline is similar to\n","scikit-learn’s pipeline concept.\n","\n","## Low-level data types\n","\n","Ngoài các kiểu cấu trúc trên, MLlib còn các các kiểu dữ liệu bậc thấp hơn (trong đó Vector là phổ biến nhất). Vector có dạng thưa và dense. \n","\n","In addition to the structural types for building pipelines, there are also several lower-level data\n","types you may need to work with in MLlib (Vector being the most common). Whenever we pass\n","a set of features into a machine learning model, we must do it as a vector that consists of\n","Doubles. This vector can be either sparse (where most of the elements are zero) or dense (where\n","there are many unique values). Vectors are created in different ways. To create a dense vector,\n","we can specify an array of all the values. To create a sparse vector, we can specify the total size\n","and the indices and values of the non-zero elements. Sparse is the best format, as you might have\n","guessed, when the majority of values are zero as this is a more compressed representation. Here\n","is an example of how to manually create a Vector:"]},{"cell_type":"code","metadata":{"id":"hB2kDYH-Q1bb"},"source":["# in Python\n","from pyspark.ml.linalg import Vectors\n","denseVec = Vectors.dense(1.0, 2.0, 3.0)\n","size = 3\n","idx = [1, 2] # locations of non-zero elements in vector\n","values = [2.0, 3.0]\n","sparseVec = Vectors.sparse(size, idx, values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuQEmPXBQ1bc","outputId":"254d00b4-8ebd-4e6a-c9b6-60a9427bbc32"},"source":["import pyspark\n","pyspark.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.0.1'"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"7pY4qWMwQ1bd","outputId":"2e1937e5-7dcb-4b4f-e890-96868d7063f4"},"source":["vec = Vectors.dense([1,2],4)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"setting an array element with a sequence.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-b84ea973efc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(*elements)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0;31m# it's list, numpy.array or other iterable object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ar)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."]}]},{"cell_type":"markdown","metadata":{"id":"bNMIBnkVQ1bd"},"source":["## MLlib in Action\n","\n","Tiếp theo là quy trình giúp minh họa các thành phần đã được giới thiệu ở trên.\n","\n","Now that we have described some of the core pieces you can expect to come across, let’s create a\n","simple pipeline to demonstrate each of the components. We’ll use a small synthetic dataset that\n","will help illustrate our point. Let’s read the data in and see a sample before talking about it\n","further:"]},{"cell_type":"code","metadata":{"id":"rqrcKqykQ1bd"},"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","sc = SparkContext('local')\n","spark = SparkSession(sc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"097T02RtQ1be","outputId":"f2dd2e6a-7678-4f1f-b9de-9ce5238d7002"},"source":["df = spark.read.json(\"../data/simple-ml\")\n","df.orderBy(\"value2\").show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+----+------+------------------+\n","|color| lab|value1|            value2|\n","+-----+----+------+------------------+\n","|green|good|     1|14.386294994851129|\n","|green| bad|    16|14.386294994851129|\n","| blue| bad|     8|14.386294994851129|\n","| blue| bad|     8|14.386294994851129|\n","| blue| bad|    12|14.386294994851129|\n","|green| bad|    16|14.386294994851129|\n","|green|good|    12|14.386294994851129|\n","|  red|good|    35|14.386294994851129|\n","|  red|good|    35|14.386294994851129|\n","|  red| bad|     2|14.386294994851129|\n","|  red| bad|    16|14.386294994851129|\n","|  red| bad|    16|14.386294994851129|\n","| blue| bad|     8|14.386294994851129|\n","|green|good|     1|14.386294994851129|\n","|green|good|    12|14.386294994851129|\n","| blue| bad|     8|14.386294994851129|\n","|  red|good|    35|14.386294994851129|\n","| blue| bad|    12|14.386294994851129|\n","|  red| bad|    16|14.386294994851129|\n","|green|good|    12|14.386294994851129|\n","+-----+----+------+------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j0K1V_LwQ1be"},"source":["## Feature Engineering with Transformers\n","\n","Với MLlib, dữ liệu đầu vào cho các thuật toán học máy phải là Double (cho label) và Vector[Double] (cho feature). Dữ liệu hiện tại chưa ở đúng kiểu dữ liệu.\n","\n","When we use MLlib, all inputs to machine learning algorithms (with several exceptions\n","discussed in later chapters) in Spark must consist of type Double (for labels) and\n","Vector[Double] (for features). The current dataset does not meet that requirement and therefore\n","we need to transform it to the proper format.\n","\n","Để đưa dữ liệu về đúng kiểu, ta sử dụng RFormular. RFormular hỗ trợ một số phép toán trong R.\n","\n","To achieve this in our example, we are going to specify an RFormula. This is a declarative\n","language for specifying machine learning transformations and is simple to use once you\n","understand the syntax. RFormula supports a limited subset of the R operators that in practice\n","work quite well for simple models and manipulations (we demonstrate the manual approach to\n","this problem in Chapter 25)"]},{"cell_type":"code","metadata":{"id":"sWniuCQJQ1be"},"source":["from pyspark.ml.feature import RFormula\n","supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qz2IhIN7Q1bf"},"source":["Ta đã chỉ rõ cách chuyển đổi dữ liệu phục vụ cho việc huấn luyện. Bước tiếp theo là \"fit\" RFormular transformer trên dữ liệu để xác định giá trị cho từng cột. RFormular tự động phát hiện các biến rời rạc cũng như các giá trị có thể có của các biến này. Transformer được trả về là sẽ được dùng để chuyển đổi dữ liệu.\n","\n","At this point, we have declaratively specified how we would like to change our data into what we\n","will train our model on. The next step is to fit the RFormula transformer to the data to let it\n","discover the possible values of each column. Not all transformers have this requirement but\n","because RFormula will automatically handle categorical variables for us, it needs to determine\n","which columns are categorical and which are not, as well as what the distinct values of the\n","categorical columns are. For this reason, we have to call the fit method. Once we call fit, it\n","returns a “trained” version of our transformer we can then use to actually transform our data."]},{"cell_type":"code","metadata":{"id":"gePhqht1Q1bg","outputId":"3eda8c6e-0c4c-4649-e9de-c04788ea928d"},"source":["fittedRF = supervised.fit(df)\n","preparedDF = fittedRF.transform(df)\n","preparedDF.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+----+------+------------------+--------------------+-----+\n","|color| lab|value1|            value2|            features|label|\n","+-----+----+------+------------------+--------------------+-----+\n","|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n","| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n","|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n","|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n","|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n","|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n","|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n","|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n","|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n","| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n","|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n","|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n","|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n","|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n","+-----+----+------+------------------+--------------------+-----+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lp0vR4eCQ1bh","outputId":"269ec79a-a2e0-4f43-d85e-2f00cbb13301"},"source":["preparedDF.select(\"features\").take(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(features=SparseVector(10, {1: 1.0, 2: 1.0, 3: 14.3863, 5: 1.0, 8: 14.3863})),\n"," Row(features=SparseVector(10, {2: 8.0, 3: 14.3863, 6: 8.0, 9: 14.3863}))]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"iBVUUIkiQ1bh"},"source":["Trong kết quả trên, có thể thấy cột \"features\" là dữ liệu đã được chuyển đổi. Dữ liệu dạng rời rạc được chuyển sang dạng one-hot. Thuốc tính của từng mẫu đươc chuyển thành sparse vector. Trước khi huấn luyện, tập dữ liệu đánh giá cần được chuẩn bị.\n","\n","In the output we can see the result of our transformation—a column called features that has our\n","previously raw data. What’s happening behind the scenes is actually pretty simple. RFormula\n","inspects our data during the fit call and outputs an object that will transform our data according\n","to the specified formula, which is called an RFormulaModel. This “trained” transformer always\n","has the word Model in the type signature. When we use this transformer, Spark automatically\n","converts our categorical variable to Doubles so that we can input it into a (yet to be specified)\n","machine learning model. In particular, it assigns a numerical value to each possible color\n","category, creates additional features for the interaction variables between colors and\n","value1/value2, and puts them all into a single vector. We then call transform on that object in\n","order to transform our input data into the expected output data.\n","\n","Thus far you (pre)processed the data and added some features along the way. Now it is time to\n","actually train a model (or a set of models) on this dataset. In order to do this, you first need to\n","prepare a test set for evaluation.\n","\n","Let’s create a simple test set based off a random split of the data now (we’ll be using this test set\n","throughout the remainder of the chapter):"]},{"cell_type":"code","metadata":{"id":"orysTrn1Q1bh"},"source":["train, test = preparedDF.randomSplit([0.7, 0.3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZ57Wy60Q1bi"},"source":["## Estimators\n","Sau khi dữ liệu đã được đưa về đúng dạng, ta có thể huấn luyện mô hình. Ta sẽ sử dụng mô hình hồi quy logistic.\n","\n","Now that we have transformed our data into the correct format and created some valuable\n","features, it’s time to actually fit our model. In this case we will use a classification algorithm\n","called logistic regression. To create our classifier we instantiate an instance of\n","LogisticRegression, using the default configuration or hyperparameters. We then set the label\n","columns and the feature columns; the column names we are setting—label and features—are\n","actually the default labels for all estimators in Spark MLlib, and in later chapters we omit them:"]},{"cell_type":"code","metadata":{"id":"5CYAeujIQ1bi"},"source":["from pyspark.ml.classification import LogisticRegression\n","lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBAa_dpOQ1bi"},"source":["Before we actually go about training this model, let’s inspect the parameters. This is also a great\n","way to remind yourself of the options available for each particular model:"]},{"cell_type":"code","metadata":{"id":"vnu7xlMiQ1bi","outputId":"699e3c89-b9df-4dc6-eb33-d2b869bf1e9a"},"source":["print(lr.explainParams())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n","elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n","family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n","featuresCol: features column name. (default: features, current: features)\n","fitIntercept: whether to fit an intercept term. (default: True)\n","labelCol: label column name. (default: label, current: label)\n","lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n","lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n","maxIter: max number of iterations (>= 0). (default: 100)\n","predictionCol: prediction column name. (default: prediction)\n","probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n","rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n","regParam: regularization parameter (>= 0). (default: 0.0)\n","standardization: whether to standardize the training features before fitting the model. (default: True)\n","threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n","thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n","tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n","upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n","upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n","weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"brG5CCFaQ1bj"},"source":["Upon instantiating an untrained algorithm, it becomes time to fit it to data. In this case, this\n","returns a LogisticRegressionModel:"]},{"cell_type":"code","metadata":{"id":"9jz4LOG4Q1bj"},"source":["fittedLR = lr.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m-HI4ykoQ1bj"},"source":["Dòng code trên khởi tạo một Spark job để huấn luyện mô hình.\n","Sau khi huấn luyện xong, ta có thể dự đoán label trên dữ liệu huấn luyện (sử dựng .transform())\n","\n","This code will kick off a Spark job to train the model. As opposed to the transformations that you\n","saw throughout the book, the fitting of a machine learning model is eager and performed\n","immediately.\n","\n","Once complete, you can use the model to make predictions. Logically this means tranforming\n","features into labels. We make predictions with the transform method. For example, we can\n","transform our training dataset to see what labels our model assigned to the training data and how\n","those compare to the true outputs. This, again, is just another DataFrame we can manipulate.\n","Let’s perform that prediction with the following code snippet:"]},{"cell_type":"code","metadata":{"id":"uH7q-FHQQ1bj","outputId":"a695b0bd-60f6-4b86-d621-9a1b799b6f18"},"source":["fittedLR.transform(train).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","|color|lab|value1|            value2|            features|label|       rawPrediction|         probability|prediction|\n","+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[97.4174094478122...|[1.0,4.9221702990...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","| blue|bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[117.337837559297...|[1.0,1.0985621445...|       0.0|\n","|green|bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[17.6362123861149...|[0.99999997808758...|       0.0|\n","|green|bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[17.6362123861149...|[0.99999997808758...|       0.0|\n","|green|bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[17.6362123861149...|[0.99999997808758...|       0.0|\n","|green|bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[17.6362123861149...|[0.99999997808758...|       0.0|\n","+-----+---+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QgkBFmT0Q1bk"},"source":["## A REVIEW OF HYPERPARAMETERS\n","\n","Hyperparameters là tham số  không được học trực tiếp trong quá trình huấn luyện, và được lựa chọn trước khi huấn luyện. Trong phần tiếp theo, ta sẽ thiết lập quy trình lựa chọn hyperparameters.\n","\n","Although we mentioned them previously, let’s more formally define hyperparameters.\n","Hyperparameters are configuration parameters that affect the training process, such as model\n","architecture and regularization. They are set prior to starting training. For instance, logistic\n","regression has a hyperparameter that determines how much regularization should be\n","performed on our data through the training phase (regularization is a technique that pushes\n","models against overfitting data). You’ll see in the next couple of pages that we can set up our\n","pipeline to try different hyperparameter values (e.g., different regularization values) in order\n","to compare different variations of the same model against one another.\n","\n","## Pipelining Our Workflow\n","\n","Trong Spark, Pipeline hỗ trợ thiết lập dataflow cho quá trình xử lý dữ liệu và huấn luyện đồng thời tự động hóa thực thi quá trình trên với các hyperparamers được cho trước.\n","\n","As you probably noticed, if you are performing a lot of transformations, writing all the steps and\n","keeping track of DataFrames ends up being quite tedious. That’s why Spark includes the\n","Pipeline concept. A pipeline allows you to set up a dataflow of the relevant transformations\n","that ends with an estimator that is automatically tuned according to your specifications, resulting\n","in a tuned model ready for use. Figure 24-4 illustrates this process.\n","![image.png](attachment:image.png)\n","\n","Chú ý các instance của transformers và model không nên sử dụng cho các pipelines khác nhau.\n","\n","Note that it is essential that instances of transformers or models are not reused across different\n","pipelines. Always create a new instance of a model before creating another pipeline.\n","\n","Để tránh overfitting, ta sẽ tạo tập holdout để đánh giá, hyperparamters được lựa chọn dựa theo kết quả trên tập validation. \n","\n","In order to make sure we don’t overfit, we are going to create a holdout test set and tune our\n","hyperparameters based on a validation set (note that we create this validation set based on the\n","original dataset, not the preparedDF used in the previous pages):"]},{"cell_type":"code","metadata":{"id":"Tn4SG1_XQ1bk"},"source":["train, test = df.randomSplit([0.7, 0.3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txMFKIoTQ1bl"},"source":["Now that you have a holdout set, let’s create the base stages in our pipeline. A stage simply\n","represents a transformer or an estimator. In our case, we will have two estimators. The RFomula\n","will first analyze our data to understand the types of input features and then transform them to\n","create new features. Subsequently, the LogisticRegression object is the algorithm that we will\n","train to produce a model:"]},{"cell_type":"code","metadata":{"id":"s-hQ6g0CQ1bl"},"source":["rForm = RFormula()\n","lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMdJXOyVQ1bl"},"source":["We will set the potential values for the RFormula in the next section. Now instead of manually\n","using our transformations and then tuning our model we just make them stages in the overall\n","pipeline, as in the following code snippet:"]},{"cell_type":"code","metadata":{"id":"RoBM4xJmQ1bm"},"source":["from pyspark.ml import Pipeline\n","stages = [rForm, lr]\n","pipeline = Pipeline().setStages(stages)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EC6psz7BQ1bm"},"source":["## Training and Evaluation\n","Sau khi thiết lập pipeline, việc tiếp theo là huấn luyện. Ta sẽ huấn luyện nhiều mô hình với hyperparameters khác nhau. mô hình được lựa chọn qua Evaluator dựa trên tập validation.\n","\n","Now that you arranged the logical pipeline, the next step is training. In our case, we won’t train\n","just one model (like we did previously); we will train several variations of the model by\n","specifying different combinations of hyperparameters that we would like Spark to test. We will\n","then select the best model using an Evaluator that compares their predictions on our validation\n","data. We can test different hyperparameters in the entire pipeline, even in the RFormula that we\n","use to manipulate the raw data. This code shows how we go about doing that:"]},{"cell_type":"code","metadata":{"id":"UxRYzjpiQ1bm"},"source":["from pyspark.ml.tuning import ParamGridBuilder\n","params = ParamGridBuilder()\\\n",".addGrid(rForm.formula, [\n","\"lab ~ . + color:value1\",\n","\"lab ~ . + color:value1 + color:value2\"])\\\n",".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",".addGrid(lr.regParam, [0.1, 2.0])\\\n",".build()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"onQWeRzcQ1bm"},"source":["Trong parameter grid trên gồm có 3 hyperparameters được hiệu chỉnh:\n","1. 2 version cho RFormula\n","2. 3 lựa chọn cho elasticnet parameter\n","3. 2 lựa chọn cho regularization parameter\n","\n","gồm có tất cả 12 cách lựa chọn 3 hyperparameters trên.\n","\n","In our current paramter grid, there are three hyperparameters that will diverge from the defaults:\n","1. Two different versions of the RFormula\n","2. Three different options for the ElasticNet parameter\n","3. Two different options for the regularization parameter\n","\n","This gives us a total of 12 different combinations of these parameters, which means we will be\n","training 12 different versions of logistic regression.\n","\n","\n","\n","\n","Now that the grid is built, it’s time to specify our evaluation process. The evaluator allows us to\n","automatically and objectively compare multiple models to the same evaluation metric. There are\n","evaluators for classification and regression, covered in later chapters, but in this case we will use\n","the BinaryClassificationEvaluator, which has a number of potential evaluation metrics, as\n","we’ll discuss in Chapter 26. In this case we will use areaUnderROC, which is the total area under\n","the receiver operating characteristic, a common measure of classification performance:"]},{"cell_type":"code","metadata":{"id":"mgHmzfJOQ1bm"},"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","# evaluator = BinaryClassificationEvaluator()\\\n","# .setMetricName(\"acc\")\\\n","# .setRawPredictionCol(\"prediction\")\\\n","# .setLabelCol(\"label\")\n","# # use probability instead of prediction\n","\n","evaluator = MulticlassClassificationEvaluator()\\\n",".setMetricName(\"f1\")\\\n",".setPredictionCol(\"prediction\")\\\n",".setLabelCol(\"label\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avaqMKipQ1bn"},"source":["Now that we have a pipeline that specifies how our data should be transformed, we will perform\n","model selection to try out different hyperparameters in our logistic regression model and\n","measure success by comparing their performance using the areaUnderROC metric.\n","\n","As we discussed, it is a best practice in machine learning to fit hyperparameters on a validation\n","set (instead of your test set) to prevent overfitting. For this reason, we cannot use our holdout test\n","set (that we created before) to tune these parameters. Luckily, Spark provides two options for\n","performing hyperparameter tuning automatically. We can use TrainValidationSplit, which\n","will simply perform an arbitrary random split of our data into two different groups, or\n","CrossValidator, which performs K-fold cross-validation by splitting the dataset into k nonoverlapping,\n","randomly partitioned folds:"]},{"cell_type":"code","metadata":{"id":"wPIH0xICQ1bn"},"source":["from pyspark.ml.tuning import TrainValidationSplit\n","tvs = TrainValidationSplit()\\\n",".setTrainRatio(0.75)\\\n",".setEstimatorParamMaps(params)\\\n",".setEstimator(pipeline)\\\n",".setEvaluator(evaluator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZZF99AA0Q1bn"},"source":["Let’s run the entire pipeline we constructed. To review, running this pipeline will test out every\n","version of the model against the validation set. Note the type of tvsFitted is\n","TrainValidationSplitModel. Any time we fit a given model, it outputs a “model” type:"]},{"cell_type":"code","metadata":{"id":"e0pJRgYnQ1bn"},"source":["tvsFitted = tvs.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mqzbp8-nQ1bn","outputId":"a7cc1ae0-543a-4949-9dd1-34f448fdb0e3"},"source":["tvsFitted.transform(test).show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","|color| lab|value1|            value2|            features|label|       rawPrediction|         probability|prediction|\n","+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[2.43088948773888...|[0.91915265620159...|       0.0|\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[2.43088948773888...|[0.91915265620159...|       0.0|\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[2.43088948773888...|[0.91915265620159...|       0.0|\n","| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|[2.43088948773888...|[0.91915265620159...|       0.0|\n","| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[2.60272038985746...|[0.93103645424062...|       0.0|\n","| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[2.60272038985746...|[0.93103645424062...|       0.0|\n","| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|[2.60272038985746...|[0.93103645424062...|       0.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[-0.4653786881391...|[0.38571062775332...|       1.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[-0.4653786881391...|[0.38571062775332...|       1.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[-0.4653786881391...|[0.38571062775332...|       1.0|\n","|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|[-0.4653786881391...|[0.38571062775332...|       1.0|\n","|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|[-0.6936935854662...|[0.33321192108054...|       1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|[-0.5262626607597...|[0.37138898408698...|       1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|[-0.5262626607597...|[0.37138898408698...|       1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|[-0.5262626607597...|[0.37138898408698...|       1.0|\n","|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|[-0.5262626607597...|[0.37138898408698...|       1.0|\n","|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|[-2.1637741561909...|[0.10305107734186...|       1.0|\n","|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|[-2.1637741561909...|[0.10305107734186...|       1.0|\n","|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|[1.71750002293740...|[0.84780654318613...|       0.0|\n","|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|[1.71750002293740...|[0.84780654318613...|       0.0|\n","+-----+----+------+------------------+--------------------+-----+--------------------+--------------------+----------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wkeeqe2GQ1bo"},"source":["And of course evaluate how it performs on the test set!"]},{"cell_type":"code","metadata":{"id":"1_9Sdfk5Q1bo","outputId":"b52efefd-f486-4c85-d738-09531710254e"},"source":["evaluator.evaluate(tvsFitted.transform(test)) // 0.9166666666666667"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"AOZbZJlsQ1bo"},"source":["## Persisting and Applying Models\n","Now that we trained this model, we can persist it to disk to use it for prediction purposes later on:"]},{"cell_type":"code","metadata":{"id":"AxorpPNhQ1bo"},"source":["tvsFitted.bestModel.write().overwrite().save(\"./\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGpuzodWQ1bo"},"source":["# Conclusion\n","Chương này đã trình bày các khái niệm cơ bản của MLlib, và cách sử dụng MLlib. Chương sau đi sâu vào tiền xử lý dữ liệu. Sau đó là các thuật toán trong MLlib và công cụ phân tích đồ thị và deep learning.\n","\n","In this chapter we covered the core concepts behind advanced analytics and MLlib. We also\n","showed you how to use them. The next chapter will discuss preprocessing in depth, including\n","Spark’s tools for feature engineering and data cleaning. Then we’ll move into detailed\n","descriptions of each algorithm available in MLlib along with some tools for graph analytics and\n","deep learning."]},{"cell_type":"code","metadata":{"id":"_ioZaR5TQ1bo"},"source":[""],"execution_count":null,"outputs":[]}]}