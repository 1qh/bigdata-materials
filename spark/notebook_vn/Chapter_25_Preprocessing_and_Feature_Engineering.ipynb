{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYhV4efglARP"
   },
   "source": [
    "# Nhóm 11\n",
    "Nguyễn Phương Thảo 20170116 \n",
    "\n",
    "Nguyễn Mai Phương 20170106\n",
    "\n",
    "Phạm Trung Kiên 20170088\n",
    "\n",
    "Trần Văn Đạo 20170051\n",
    "# Formatting Models According to Your Use Case \n",
    "\n",
    "Để tiền xử lý dữ liệu cho các công cụ phân tích nâng cao khác nhau của Spark, bạn phải xem xét mục tiêu cuối cùng của mình. Sau đây là một số các yêu cầu đối với cấu trúc dữ liệu đầu vào cho từng tác vụ phân tích nâng cao trong MLlib:\n",
    "\n",
    "1. Trong hầu hết các thuật toán phân loại và hồi quy, bạn muốn đưa dữ liệu của mình vào trong một cột có loại Double để đại diện cho nhãn và một cột loại Vector (có thể dày hoặc thưa) để đại diện cho các features.\n",
    "2. Trong trường hợp được đề xuất, bạn muốn đưa dữ liệu của mình vào cột người dùng, cột mục (chẳng hạn như phim hoặc sách) và cột xếp hạng.\n",
    "3. Trong trường hợp học không có giám sát, một cột kiểu Vector (dày hoặc thưa) là cần thiết để đại diện cho các features.\n",
    "4. Trong trường hợp phân tích đồ thị, bạn sẽ cần một DataFrame của các đỉnh và một DataFrame của các cạnh.\n",
    "\n",
    "Cách tốt nhất để lấy dữ liệu ở các định dạng này là thông qua transformers. Transformers là tập các hàm nhận một DataFrame làm đối số và trả về một DataFrame mới. Chương này sẽ tập trung vào những transformer nào có liên quan đến các trường hợp sử dụng cụ thể hơn là cố gắng liệt kê mọi transformers có thể.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "Spark cung cấp một số transformer như một phần của package org.apache.spark.ml.feature. Package tương ứng trong Python là pyspark.ml.feature. Những transformer mới liên tục xuất hiện trong Spark MLlib nên sẽ không có danh sách chính xác trong tài liệu dưới đây. Thông tin mới nhất có thể được tìm thấy trên Spark website.\n",
    "\n",
    "Trước khi tiếp tục, chúng ta sẽ đọc một số tập dữ liệu mẫu khác nhau, mỗi tập dữ liệu có các thuộc tính khác nhau mà chúng ta sẽ thao tác trong chương này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3ZJPg7AlARS"
   },
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"../data/retail-data/by-day/*.csv\")\\\n",
    ".coalesce(5)\\\n",
    ".where(\"Description IS NOT NULL\")\n",
    "#fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"../data/simple-ml\")\n",
    "#scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIQZcmr1lART"
   },
   "outputs": [],
   "source": [
    "scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPKVeJBllART"
   },
   "outputs": [],
   "source": [
    "fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIYPZp91lART"
   },
   "source": [
    "Ngoài dữ liệu bán hàng thực tế này, chúng tôi cũng sẽ sử dụng một số bộ dữ liệu tổng hợp đơn giản. FakeIntDF, simpleDF và scaleDF đều có rất ít hàng. Điều này sẽ cung cấp cho bạn khả năng tập trung vào thao tác dữ liệu chính xác mà chúng tôi đang thực hiện thay vì sự mâu thuẫn khác nhau của bất kỳ tập dữ liệu cụ thể nào. Bởi vì chúng tôi sẽ truy cập vào dữ liệu bán hàng một vài lần, chúng tôi sẽ lưu vào bộ nhớ cache để có thể đọc dữ liệu đó một cách hiệu quả từ bộ nhớ thay vì đọc từ đĩa mỗi khi chúng tôi cần. Hãy cũng kiểm tra một số hàng dữ liệu đầu tiên để hiểu rõ hơn về những gì có trong tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG81zLU-lART",
    "outputId": "5c5a3a6c-7e51-442d-9120-f0e045de8e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.cache()\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUw_8HxklARW"
   },
   "source": [
    "## Transformers\n",
    "\n",
    "Chúng ta đã thảo luận về transformers trong chương trước, nhưng chúng ta có thể review nó ở đây. Transformers là tập các hàm thực hiện convert dữ liệu thô theo một cách nào đó. Điều này có thể là để tạo một biến tương tác mới (từ hai biến khác), để chuẩn hóa một cột hoặc chỉ đơn giản là biến nó thành một Double để trở thành input của một mô hình. Transformers chủ yếu được sử dụng trong quá trình tiền xử lý hoặc tạo tính năng.\n",
    "\n",
    "Transformer của Spark chỉ bao gồm một phương thức biến đổi do nó sẽ không thay đổi theo dữ liệu đầu vào.\n",
    "\n",
    "Tokenizer là một ví dụ về transformer. Nó tokenize một chuỗi, phân tách các ký tự và không có gì để học từ dữ liệu; nó chỉ đơn giản là áp dụng một hàm. Chúng ta sẽ thảo luận sâu hơn về tokenizer ở phần sau của chương này, nhưng đây là một đoạn mã nhỏ cho thấy cách tokenizer được xây dựng để accept cột đầu vào, cách nó biến đổi dữ liệu và sau đó là kết quả đầu ra từ biến đổi đó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g21cLtAWlARW"
   },
   "outputs": [],
   "source": [
    "#// in Scala\n",
    "#import org.apache.spark.ml.feature.Tokenizer\n",
    "#val tkn = new Tokenizer().setInputCol(\"Description\")\n",
    "#tkn.transform(sales.select(\"Description\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpG82iZalARX"
   },
   "source": [
    "## Estimators for Preprocessing\n",
    "\n",
    "Một công cụ khác để tiền xử lý là các estimators. Estimators là cần thiết khi một phép biến đổi bạn muốn thực hiện phải được khởi tạo bằng dữ liệu hoặc thông tin về cột đầu vào (thường được lấy bằng cách thực hiện pass qua chính cột đầu vào). Ví dụ: nếu bạn muốn chia tỷ lệ các giá trị trong cột để có giá trị trung bình bằng 0 và phương sai đơn vị, bạn sẽ cần thực hiện chuyển trên toàn bộ dữ liệu để tính toán các giá trị bạn sẽ sử dụng để chuẩn hóa dữ liệu. Trên thực tế, estimators có thể là một transformer được cấu hình theo dữ liệu đầu vào cụ thể của bạn. Nói một cách đơn giản nhất, bạn có thể áp dụng một cách mù quáng một phép biến đổi (loại transformer “thông thường”) hoặc thực hiện một phép biến đổi dựa trên dữ liệu của bạn (loại estimator). Hình 25- 2 là một minh họa đơn giản về một ước lượng phù hợp với một tập dữ liệu đầu vào cụ thể, tạo ra một transformer sau đó áp dụng cho tập dữ liệu đầu vào để thêm một cột mới (của dữ liệu đã biến đổi).\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Một ví dụ về loại estimator này là StandardScaler, chia tỷ lệ cột đầu vào của bạn theo phạm vi giá trị trong cột đó để có giá trị trung bình bằng 0 và phương sai là 1 trong mỗi dimension. Vì lý do đó, trước tiên nó phải thực hiện biến đổi dữ liệu để tạo transformer. Đây là đoạn code mẫu hiển thị toàn bộ quá trình cũng như kết quả đầu ra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2L7PEGqlARX"
   },
   "outputs": [],
   "source": [
    "#// in Scala\n",
    "#import org.apache.spark.ml.feature.StandardScaler\n",
    "#val ss = new StandardScaler().setInputCol(\"features\")\n",
    "#ss.fit(scaleDF).transform(scaleDF).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNg38dtPlARX"
   },
   "source": [
    "## Transformer Properties\n",
    "\n",
    "Tất cả các transformer đều yêu cầu bạn chỉ định ở mức tối thiểu inputCol và outputCol, đại diện cho tên cột của đầu vào và đầu ra tương ứng. Bạn đặt chúng với setInputCol và setOutputCol. Có một số giá trị mặc định (bạn có thể tìm thấy chúng trong tài liệu), nhưng cách tốt nhất là bạn nên tự chỉ định chúng theo cách thủ công cho rõ ràng. Ngoài các cột đầu vào và đầu ra, tất cả các transformer đều có các thông số khác nhau mà bạn có thể điều chỉnh (bất cứ khi nào chúng tôi đề cập đến một thông số trong chương này, bạn phải đặt lại nó bằng phương thức set ()). Trong Python, chúng tôi cũng có một phương pháp khác để đặt các giá trị này bằng các keyword argument cho phương thức khởi tạo của đối tượng. Chúng tôi loại trừ chúng khỏi các ví dụ trong chương tiếp theo để nhất quán. Estimator yêu cầu bạn điều chỉnh các transformer với tập dữ liệu cụ thể của bạn và sau đó biến đổi trên đối tượng kết quả\n",
    "\n",
    "## NOTE\n",
    "\n",
    "Spark MLlib lưu trữ siêu dữ liệu metadata về các cột mà nó sử dụng trong mỗi DataFrame dưới dạng một thuộc tính trên chính cột đó. Điều này cho phép nó lưu trữ (và chú thích) một cách chính xác rằng một cột Double có thể thực sự đại diện cho một loạt các biến phân loại thay vì các giá trị liên tục. Tuy nhiên, siêu dữ liệu sẽ không hiển thị khi bạn in biểu đồ hoặc DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gKbkztilARX"
   },
   "source": [
    "# High-Level Transformers\n",
    "\n",
    "High-level transformers, chẳng hạn như RFormula mà chúng ta đã thấy trong chương trước, cho phép bạn chỉ định ngắn gọn một số phép biến đổi trong một biến đổi. Chúng hoạt động ở “mức độ cao” và cho phép bạn tránh phải thực hiện từng thao tác hoặc biến đổi dữ liệu. Nói chung, bạn nên cố gắng sử dụng các transformer mức cao nhất có thể, để giảm thiểu rủi ro lỗi và giúp bạn tập trung vào vấn đề thay vì các chi tiết nhỏ hơn trong quá trình thực hiện. Mặc dù điều này không phải lúc nào cũng khả thi, nhưng đó là một mục tiêu tốt.\n",
    "\n",
    "## RFormula\n",
    "\n",
    "RFormula là transformer dễ sử dụng nhất khi bạn có dữ liệu đã được định dạng \"thông thường\". Spark mượn transformer này từ ngôn ngữ R để làm cho việc chỉ định khai báo một tập hợp các biến đổi cho dữ liệu của bạn trở nên đơn giản. Với transformer này, các giá trị có thể là số hoặc categorical và bạn không cần phải trích xuất giá trị từ chuỗi hoặc thao tác chúng theo bất kỳ cách nào. RFormula sẽ tự động xử lý các đầu vào categorical (được chỉ định dưới dạng chuỗi) bằng cách thực hiện một mã hóa one-hot encoding. Tóm lại, one-hot encoding biến đổi một tập hợp các giá trị thành một tập hợp các cột nhị phân xác định xem điểm dữ liệu có từng giá trị cụ thể hay không (chúng ta sẽ thảo luận sâu hơn về one-hot encoding ở phần sau của chương). Với RFormula, các cột số sẽ được chuyển thành Double nhưng sẽ không được mã hóa one-hot. Nếu cột nhãn thuộc loại String, đầu tiên nó sẽ được chuyển thành Double với StringIndexer\n",
    "\n",
    "### WARNING\n",
    "Tự động truyền các cột số thành Double mà không cần mã hóa one-hot có một số ý nghĩa quan trọng. Nếu bạn có các biến phân loại categorical có giá trị số, chúng sẽ chỉ được chuyển thành Double, ngầm định một thứ tự. Điều quan trọng là đảm bảo các loại đầu vào tương ứng với biến đổi dự kiến. Nếu bạn có các biến categorical thực sự không có quan hệ thứ tự, chúng sẽ được chuyển thành Chuỗi. Bạn cũng có thể lập chỉ mục các cột theo cách thủ công (xem “Working with Categorical Features”)\n",
    "\n",
    "RFormula cho phép bạn chỉ định các phép biến đổi của mình trong cú pháp khai báo. Nó rất đơn giản để sử dụng một khi bạn hiểu cú pháp. Hiện tại, RFormula hỗ trợ một tập con giới hạn của các toán tử R mà trên thực tế hoạt động khá tốt cho các phép biến đổi đơn giản. Các toán tử cơ bản là\n",
    "1. ~ riêng biệt target và các terms\n",
    "2. + Nối các term; “+ 0” có nghĩa là loại bỏ intercept (điều này có nghĩa là intercept y của đường mà chúng ta sắp xếp sẽ là 0)\n",
    "3. - Xóa bỏ một term; “- 1” có nghĩa là loại bỏ intercept (điều này có nghĩa là intercept y của đường mà chúng ta sắp xếp sẽ là 0)\n",
    "4. : Interaction (phép nhân cho các giá trị số hoặc các giá trị phân loại được biến đổi thành giá trị nhị phân)\n",
    "5. . Tất cả các cột ngoại trừ biến mục tiêu / phụ thuộc\n",
    "\n",
    "RFormula cũng sử dụng các cột nhãn và tính năng mặc định để gắn nhãn, bạn đoán nó, nhãn và tập hợp các tính năng mà nó xuất ra (đối với học có giám sát). Các nodel được đề cập ở phần sau của chương này theo yêu cầu mặc định các tên cột đó, giúp dễ dàng chuyển DataFrame đã chuyển đổi kết quả thành một mô hình để thực hiện training. Nếu điều này vẫn chưa hợp lý, đừng lo lắng— điều đó sẽ trở nên rõ ràng khi chúng ta thực sự bắt đầu sử dụng các model trong các chương sau.\n",
    "\n",
    "Sử dụng RFormula trong một ví dụ sau. Trong trường hợp này, chúng tôi muốn sử dụng tất cả các biến có sẵn (.) Và sau đó chỉ định tương tác giữa value1 và color, value2 và color làm các feature bổ sung để generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0YNO1ZUlARX",
    "outputId": "f281af95-0f0c-45d3-d329-de631527e680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A39jJztLlARX"
   },
   "source": [
    "## SQL Transformers\n",
    "SQLTransformer cho phép bạn tận dụng thư viện khổng lồ của Spark về các thao tác liên quan đến SQL giống như cách bạn thực hiện một phép chuyển đổi MLlib. Bất kỳ câu lệnh SELECT nào bạn có thể sử dụng trong SQL đều là một phép biến đổi hợp lệ. Điều duy nhất bạn cần thay đổi là thay vì sử dụng tên bảng, bạn chỉ nên sử dụng từ khóa TH IS. Bạn có thể sử dụng SQLTransformer nếu bạn muốn mã hóa một số thao tác DataFrame như một bước tiền xử lý hoặc thử các biểu thức SQL khác nhau cho các tính năng trong quá trình điều chỉnh siêu tham số metadata. Cũng lưu ý rằng đầu ra của chuyển đổi này sẽ được nối dưới dạng một cột vào DataFrame đầu ra.\n",
    "\n",
    "Bạn có thể sử dụng SQLTransformer để thể hiện tất cả các thao tác của bạn trên dạng dữ liệu thô sơ nhất để bạn có thể version các biến thể khác nhau của thao tác dưới dạng transformer. Điều này mang lại cho bạn lợi ích từ việc xây dựng và thử nghiệm các pipelines khác nhau, tất cả chỉ bằng cách hoán đổi các transformer. Sau đây là một ví dụ cơ bản về việc sử dụng SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nlTR8l3lARX",
    "outputId": "22c17e8b-7419-42f5-8d2a-0d686312f6ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "|          854|     117|   17884.0|\n",
      "|           97|      12|   16596.0|\n",
      "|          290|      98|   13607.0|\n",
      "|          541|      27|   14285.0|\n",
      "|          244|      31|   16561.0|\n",
      "|          491|     152|   13956.0|\n",
      "|          204|      76|   13533.0|\n",
      "|          493|      64|   16629.0|\n",
      "|          159|      38|   17267.0|\n",
      "|         1140|      30|   13918.0|\n",
      "|           55|      28|   18114.0|\n",
      "|           88|       7|   14473.0|\n",
      "|          150|      16|   14024.0|\n",
      "|          206|      23|   12493.0|\n",
      "|          138|      18|   15776.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "basicTransformation = SQLTransformer()\\\n",
    ".setStatement(\"\"\"\n",
    "SELECT sum(Quantity), count(*), CustomerID\n",
    "FROM __THIS__\n",
    "GROUP BY CustomerID\n",
    "\"\"\")\n",
    "basicTransformation.transform(sales).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GupWWwQklARY"
   },
   "source": [
    "## VectorAssembler\n",
    "VectorAssembler là một công cụ bạn sẽ sử dụng trong hầu hết mọi pipeline bạn generate. Nó giúp kết hợp tất cả các feature của bạn thành một vector lớn mà bạn có thể chuyển vào một estimator. Nó thường được sử dụng trong bước cuối cùng của quy trình học máy và nhận làm đầu vào một số cột Boolean, Double hoặc Vector. Điều này đặc biệt hữu ích nếu bạn định thực hiện một số thao tác sử dụng nhiều loại transformer khác nhau và cần tập hợp tất cả các kết quả đó lại với nhau. Đầu ra từ đoạn mã sau sẽ làm rõ cách hoạt động của điều này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmZ6hX-PlARY",
    "outputId": "e2ad82bc-3c68-40d2-ab54-f2ab3aa9cf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_6bae49278a4e__output|\n",
      "+----+----+----+------------------------------------+\n",
      "|   1|   2|   3|                       [1.0,2.0,3.0]|\n",
      "|   7|   8|   9|                       [7.0,8.0,9.0]|\n",
      "|   4|   5|   6|                       [4.0,5.0,6.0]|\n",
      "+----+----+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwkmrxj9lARY"
   },
   "source": [
    "# Working with Continuous Features\n",
    "Các continuos feature chỉ là các giá trị trên trục số, từ dương vô cùng đến âm vô cùng. Có hai transformer chung cho các tính năng liên tục. Trước tiên, bạn có thể chuyển đổi các continuos feature thành các feature phân loại thông qua một quy trình được gọi là bucketing hoặc bạn có thể mở rộng quy mô và chuẩn hóa các features của mình theo một số yêu cầu khác nhau. Các transformer này sẽ chỉ hoạt động trên loại Double, vì vậy hãy đảm bảo rằng bạn đã chuyển mọi giá trị số khác thành Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dooWplpelARY"
   },
   "outputs": [],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yaa64mrGlARY"
   },
   "source": [
    "## Bucketing\n",
    "Cách tiếp cận đơn giản nhất để bucketing hoặc binning là sử dụng Bucketizer. Điều này sẽ chia một continuos feature nhất định thành các nhóm mà bạn chỉ định. Bạn chỉ định cách tạo nhóm thông qua một mảng hoặc list các giá trị Double. Điều này hữu ích vì bạn có thể muốn đơn giản hóa các tính năng trong tập dữ liệu của mình hoặc đơn giản hóa các biểu diễn của chúng để diễn giải sau này. Ví dụ: hãy tưởng tượng bạn có một cột đại diện cho cân nặng của một người và bạn muốn dự đoán một số giá trị dựa trên thông tin này. Trong một số trường hợp, có thể đơn giản hơn nếu tạo ba nhóm “overweight”, “average” và “underweight”.\n",
    "\n",
    "Để chỉ định bucket, có thể set các border của nó. Ví dụ: phân chia thành 5.0, 10.0, 250.0 trên contDF của chúng tôi sẽ thực sự không thành công vì chúng tôi không cover được tất cả các phạm vi đầu vào có thể có. Khi chỉ định các bucket point của bạn, các giá trị bạn chuyển vào các phần phân tách phải đáp ứng ba yêu cầu:\n",
    "\n",
    "1. Giá trị tối thiểu trong mảng phân tách của bạn phải nhỏ hơn giá trị tối thiểu trong DataFrame của bạn.\n",
    "2. Giá trị tối đa trong mảng phân tách của bạn phải lớn hơn giá trị lớn nhất trong DataFrame của bạn.\n",
    "3. Bạn cần chỉ định tối thiểu ba giá trị trong mảng phân tách, mảng này tạo ra hai buckets.\n",
    "\n",
    "Để cover tất cả các phạm vi có thể có, scala.Double.NegativeInfinity có thể là một option phân tách khác, với scala.Double.PositiveInfinity để bao gồm tất cả các phạm vi có thể có bên ngoài các phần bên trong. Trong Python, chúng tôi chỉ định điều này theo cách sau: float(\"inf\"), float(\"-inf\")\n",
    "\n",
    "Để xử lý các giá trị null hoặc NaN, chúng ta phải chỉ định tham số handleInvalid là một giá trị mặc định. Chúng ta có thể giữ các giá trị đó (giữ lại), error hoặc null, hoặc bỏ qua các hàng chứa giá trị đó. Đây là một ví dụ về việc sử dụng bucketing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpLpWJXwlARY",
    "outputId": "ced6b218-0b84-48b2-ac64-1f1261ad8744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_54204d279542__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            0.0|\n",
      "| 2.0|                            0.0|\n",
      "| 3.0|                            0.0|\n",
      "| 4.0|                            0.0|\n",
      "| 5.0|                            1.0|\n",
      "| 6.0|                            1.0|\n",
      "| 7.0|                            1.0|\n",
      "| 8.0|                            1.0|\n",
      "| 9.0|                            1.0|\n",
      "|10.0|                            2.0|\n",
      "|11.0|                            2.0|\n",
      "|12.0|                            2.0|\n",
      "|13.0|                            2.0|\n",
      "|14.0|                            2.0|\n",
      "|15.0|                            2.0|\n",
      "|16.0|                            2.0|\n",
      "|17.0|                            2.0|\n",
      "|18.0|                            2.0|\n",
      "|19.0|                            2.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0USTbCURlARZ"
   },
   "source": [
    "Ngoài việc phân tách dựa trên các giá trị được mã hóa cứng, một option khác là phân tách dựa trên các phân vị trong dữ liệu của chúng tôi. Điều này được thực hiện với QuantileDiscretizer, sẽ đưa các giá trị vào các bucket do người dùng chỉ định với các phần tách được xác định bởi các giá trị lượng tử gần đúng. Ví dụ: lượng tử thứ 90 là điểm trong dữ liệu của bạn tại đó 90% dữ liệu nằm dưới giá trị đó. Bạn có thể kiểm soát mức độ phân chia của các nhóm bằng cách đặt sai số tương đối cho phép tính lượng tử gần đúng bằng cách sử dụng setRelativeError. Spark thực hiện điều này bằng cách cho phép bạn chỉ định số lượng buckets bạn muốn từ dữ liệu và nó sẽ chia nhỏ dữ liệu của bạn cho phù hợp. Sau đây là một ví dụ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KMIkrWWlARZ",
    "outputId": "d158dc9b-baba-42d2-f375-3eb94d4f9d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+\n",
      "|  id|QuantileDiscretizer_0955c987ed61__output|\n",
      "+----+----------------------------------------+\n",
      "| 0.0|                                     0.0|\n",
      "| 1.0|                                     0.0|\n",
      "| 2.0|                                     0.0|\n",
      "| 3.0|                                     1.0|\n",
      "| 4.0|                                     1.0|\n",
      "| 5.0|                                     1.0|\n",
      "| 6.0|                                     1.0|\n",
      "| 7.0|                                     2.0|\n",
      "| 8.0|                                     2.0|\n",
      "| 9.0|                                     2.0|\n",
      "|10.0|                                     2.0|\n",
      "|11.0|                                     3.0|\n",
      "|12.0|                                     3.0|\n",
      "|13.0|                                     3.0|\n",
      "|14.0|                                     3.0|\n",
      "|15.0|                                     4.0|\n",
      "|16.0|                                     4.0|\n",
      "|17.0|                                     4.0|\n",
      "|18.0|                                     4.0|\n",
      "|19.0|                                     4.0|\n",
      "+----+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUOS2eQclARZ"
   },
   "source": [
    "## Advanced bucketing techniques\n",
    "\n",
    "Các kỹ thuật được mô tả ở đây là những cách phổ biến nhất để bucketing dữ liệu , nhưng có một số cách khác tồn tại trong Spark. Tất cả các quy trình này đều giống nhau từ góc độ luồng dữ liệu: bắt đầu với dữ liệu liên tục và đặt chúng trong các nhóm để chúng trở nên rời rạc. Sự khác biệt phát sinh tùy thuộc vào thuật toán được sử dụng để tính toán các nhóm này. Các ví dụ đơn giản mà chúng ta vừa xem qua rất dễ hiểu và làm việc, nhưng các kỹ thuật nâng cao hơn như locality sensitivity hashing (LSH) cũng có sẵn trong MLlib\n",
    "\n",
    "# Scaling and Normalization\n",
    "\n",
    "Chúng tôi đã thấy cách chúng tôi có thể sử dụng bucketing để tạo nhóm từ các biến liên tục. Một nhiệm vụ phổ biến khác là mở rộng quy mô và chuẩn hóa dữ liệu liên tục. Mặc dù không phải lúc nào cũng cần thiết, nhưng làm như vậy thường là cách tốt nhất. Bạn có thể muốn làm điều này khi dữ liệu của bạn chứa một số cột dựa trên các tỷ lệ khác nhau\n",
    "\n",
    "## StandardScaler\n",
    "\n",
    "StandardScaler chuẩn hóa một tập hợp các feature để có giá trị trung bình bằng 0 và độ lệch chuẩn là 1. Cờ withStd sẽ chia tỷ lệ dữ liệu thành độ lệch chuẩn đơn vị trong khi cờ withMean (sai so với mặc định) sẽ căn giữa dữ liệu trước khi chia tỷ lệ.\n",
    "\n",
    "### WARNING\n",
    "Căn giữa có thể rất tốn kém trên các vector thưa vì nó thường biến chúng thành các vectơ dày, vì vậy hãy cẩn thận trước khi căn giữa dữ liệu của bạn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR8c5pkLlARZ",
    "outputId": "5fa98ea7-164f-499c-f1a1-454fadb2a966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_fb93b8ee2825__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhvzKMIwlARZ"
   },
   "source": [
    "## MinMaxScaler\n",
    "MinMaxScaler sẽ chia tỷ lệ các giá trị trong một vector thành các giá trị tỷ lệ trên thang từ giá trị tối thiểu nhất định đến giá trị tối đa. Nếu bạn chỉ định giá trị nhỏ nhất là 0 và giá trị lớn nhất là 1, thì tất cả các giá trị sẽ nằm trong khoảng từ 0 đến 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N3pi0k1lARZ",
    "outputId": "50740493-b247-427e-8535-366bc7db5536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_b893d8ac3bef__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-0eJ5iOlARa"
   },
   "source": [
    "## MaxAbsScaler\n",
    "MaxAbsScaler chia tỷ lệ dữ liệu bằng cách chia từng giá trị cho giá trị tối đa của giá trị tuyệt đối trong feature này. Do đó, tất cả các giá trị đều nằm trong khoảng từ −1 đến 1. Transformer này hoàn toàn không thay đổi hoặc căn giữa dữ liệu trong quá trình: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3KIg85vlARa",
    "outputId": "d385a60e-c737-447d-b8c4-4850f85fc051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_8505d1d5fdab__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrx91OtolARa"
   },
   "source": [
    "## ElementwiseProduct\n",
    "ElementwiseProduct cho phép chúng tôi chia tỷ lệ từng giá trị trong một vector theo một giá trị tùy ý. Ví dụ: với vectơ bên dưới và hàng “1, 0,1, -1”, đầu ra sẽ là “10, 1,5, -20”. Đương nhiên, kích thước của vector tỷ lệ phải khớp với kích thước của vector bên trong cột có liên quan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYj5WkMTlARa",
    "outputId": "e38dcbf5-acea-403a-c8f5-a229fb3e5867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_cecc9720407d__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    ".setScalingVec(scaleUpVec)\\\n",
    ".setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIaoNB94lARb"
   },
   "source": [
    "## Normalizer\n",
    "\n",
    "Bộ chuẩn hóa cho phép chúng ta chia tỷ lệ các vector đa chiều bằng cách sử dụng một trong số các power norm, được đặt thông qua tham số “p”. Ví dụ, chúng ta có thể sử dụng chuẩn Manhattan (hoặc khoảng cách Manhattan) với p = 1, chuẩn Euclid với p = 2, v.v. Khoảng cách Manhattan là thước đo khoảng cách mà bạn chỉ có thể đi từ điểm này đến điểm khác dọc theo các đường thẳng của trục (như các đường phố ở Manhattan).\n",
    "Đây là một ví dụ về việc sử dụng Normalizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNndXz3_lARb",
    "outputId": "978a71f9-3e0e-4d50-f06a-b1bb1c65943f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_e6de09c9482d__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgch5jzUlARc"
   },
   "source": [
    "# Working with Categorical Features\n",
    "\n",
    "Nhiệm vụ phổ biến nhất đối với các categorical features là lập chỉ mục. Lập chỉ mục chuyển đổi một biến phân loại trong cột thành một biến số mà bạn có thể đưa vào các thuật toán học máy. Mặc dù điều này là đơn giản về mặt khái niệm, nhưng có một số điều quan trọng cần được ghi nhớ để Spark có thể thực hiện điều này một cách ổn định và có thể lặp lại.\n",
    "\n",
    "Nói chung, chúng tôi khuyên bạn nên lập chỉ mục lại mọi biến phân loại khi tiền xử lý chỉ vì mục đích nhất quán. Điều này có thể hữu ích trong việc duy trì các mô hình của bạn về lâu dài vì các phương pháp mã hóa của bạn có thể thay đổi theo thời gian\n",
    "\n",
    "## StringIndexer\n",
    "\n",
    "Cách đơn giản nhất để lập chỉ mục là thông qua StringIndexer, ánh xạ các chuỗi thành các số ID khác nhau. StringIndexer cũng tạo siêu dữ liệu metadata gắn với DataFrame để chỉ định đầu vào nào tương ứng với đầu ra nào. Điều này cho phép chúng tôi sau đó lấy lại đầu vào từ các giá trị chỉ mục tương ứng của chúng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roW1sKgelARc",
    "outputId": "1c77fac6-d69e-4e01-97e2-d19255f71932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yekzcyvelARc"
   },
   "source": [
    "\n",
    "Hãy nhớ rằng StringIndexer là một estimator phải phù hợp với dữ liệu đầu vào. Điều này có nghĩa là nó phải xem xét tất cả các đầu vào để chọn ánh xạ các đầu vào thành ID. Nếu bạn train một StringIndexer trên các đầu vào “a,” “b,” và “c” và sau đó sử dụng nó với đầu vào “d”, nó sẽ xuất hiện một lỗi theo mặc định. Một option khác là bỏ qua toàn bộ hàng nếu giá trị đầu vào không phải là giá trị được xem xét trong quá trình huấn luyện. Cùng với ví dụ trước, giá trị đầu vào là “d” sẽ khiến hàng đó bị bỏ qua hoàn toàn. Chúng tôi có thể đặt option này trước hoặc sau khi training trình chỉ mục hoặc pipeline. Nhiều option hơn có thể được thêm vào tính năng này trong tương lai nhưng kể từ Spark 2.2, bạn chỉ có thể bỏ qua hoặc gặp lỗi trên các đầu vào không hợp lệ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQei2mgXlARc"
   },
   "outputs": [],
   "source": [
    "# valIndexer.setHandleInvalid(\"skip\")\n",
    "# valIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFJCRmUNlARc"
   },
   "source": [
    "## Converting Indexed Values Back to Text\n",
    "\n",
    "Khi kiểm tra kết quả của quá trình học máy, bạn có thể muốn ánh xạ lại các giá trị ban đầu. Vì các mô hình phân loại MLlib đưa ra dự đoán bằng cách sử dụng các giá trị được lập chỉ mục, nên việc chuyển đổi này rất hữu ích để chuyển đổi các dự đoán của mô hình (chỉ số) trở lại các danh mục ban đầu. Chúng ta có thể làm điều này với IndexToString. Bạn sẽ nhận thấy rằng chúng tôi không phải nhập giá trị của mình vào key String; Spark’s MLlib duy trì siêu dữ liệu này cho bạn. Bạn có thể chọn chỉ định đầu ra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hjjS31hlARc",
    "outputId": "36761be9-91a1-4213-f223-5a9a311a1c92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|color| lab|value1|            value2|labelInd|IndexToString_c1e439e978f6__output|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|     1|14.386294994851129|     1.0|                              good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|                               bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|                               bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|                              good|\n",
      "|green|good|    12|14.386294994851129|     1.0|                              good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|                               bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|                              good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|                               bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|                               bad|\n",
      "+-----+----+------+------------------+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\")\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZmX2hwNlARd"
   },
   "source": [
    "## Indexing in Vectors\n",
    "\n",
    "VectorIndexer là một công cụ hữu ích để làm việc với các biến phân loại đã được tìm thấy bên trong vectơ trong tập dữ liệu của bạn. Công cụ này sẽ tự động tìm các đối tượng phân loại bên trong vectơ đầu vào của bạn và chuyển chúng thành các đối tượng phân loại với các chỉ số danh mục dựa trên số không. Ví dụ: trong DataFrame sau, cột đầu tiên trong Vectơ của chúng tôi là một biến phân loại với hai danh mục khác nhau trong khi phần còn lại của các biến là liên tục. Bằng cách đặt maxCategories thành 2 trong VectorIndexer của chúng tôi, chúng tôi đang hướng dẫn Spark lấy bất kỳ cột nào trong vectơ của chúng tôi có hai hoặc ít giá trị khác biệt hơn và chuyển đổi nó thành một biến phân loại. Điều này có thể hữu ích khi bạn biết có bao nhiêu giá trị duy nhất trong danh mục lớn nhất của mình vì bạn có thể chỉ định điều này và nó sẽ tự động lập chỉ mục các giá trị tương ứng. Ngược lại, Spark thay đổi dữ liệu dựa trên thông số này, vì vậy nếu bạn có các biến liên tục không xuất hiện đặc biệt liên tục (nhiều giá trị lặp lại), chúng có thể được chuyển đổi một cách không chủ ý thành biến phân loại nếu có quá ít giá trị duy nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNPcIs7PlARd",
    "outputId": "d5aca4d1-9f11-4aef-f6f2-d599259dff79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "idxIn = spark.createDataFrame([\n",
    "(Vectors.dense(1, 2, 3),1),\n",
    "(Vectors.dense(2, 5, 6),2),\n",
    "(Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "indxr = VectorIndexer()\\\n",
    ".setInputCol(\"features\")\\\n",
    ".setOutputCol(\"idxed\")\\\n",
    ".setMaxCategories(2)\n",
    "indxr.fit(idxIn).transform(idxIn).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLw3NMOAlARd"
   },
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Lập chỉ mục các biến phân loại chỉ là một nửa của câu chuyện. Mã hóa one-hot là một phép biến đổi dữ liệu cực kỳ phổ biến được thực hiện sau khi lập chỉ mục các biến phân loại. Điều này là do lập chỉ mục không phải lúc nào cũng đại diện cho các biến phân loại của chúng ta theo cách chính xác để các downstream models xử lý. Ví dụ: khi chúng tôi lập chỉ mục cho cột \"color\" của mình, bạn sẽ nhận thấy rằng một số màu có giá trị (hoặc số chỉ mục) cao hơn những màu khác (trong trường hợp của chúng tôi, màu xanh lam blue là 1 và xanh lục green là 2)\n",
    "\n",
    "Điều này không chính xác vì nó tạo ra biểu diễn toán học mà đầu vào cho thuật toán học máy dường như chỉ định rằng màu xanh lá cây green > màu xanh lam blue, điều này không có ý nghĩa gì trong trường hợp của các categories hiện tại. Để tránh điều này, chúng tôi sử dụng OneHotEncoder, sẽ chuyển đổi từng giá trị riêng biệt thành cờ Boolean (1 hoặc 0) như một thành phần trong vectơ. Khi chúng tôi mã hóa giá trị màu, chúng tôi có thể thấy những giá trị này không còn được sắp xếp theo thứ tự nữa, giúp chúng dễ dàng hơn cho các downstream model (ví dụ: mô hình tuyến tính)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v2VmhHSlARd",
    "outputId": "ebc5d990-7247-4994-e27a-07a3595326fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------------------------------+\n",
      "|color|colorInd|OneHotEncoder_1578c006c426__output|\n",
      "+-----+--------+----------------------------------+\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "| blue|     2.0|                         (2,[],[])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|green|     1.0|                     (2,[1],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "|  red|     0.0|                     (2,[0],[1.0])|\n",
      "+-----+--------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "ohe = ohe.fit(colorLab)\n",
    "ohe.transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "157XLIAylARd"
   },
   "source": [
    "# Text Data Transformers\n",
    "\n",
    "Văn bản luôn là đầu vào phức tạp vì nó thường đòi hỏi nhiều thao tác để ánh xạ tới định dạng mà mô hình học máy sẽ có thể sử dụng hiệu quả. Nói chung, bạn sẽ thấy hai loại văn bản: văn bản dạng tự do và biến phân loại chuỗi. Phần này chủ yếu tập trung vào văn bản dạng tự do vì chúng ta đã thảo luận về các biến phân loại\n",
    "\n",
    "## Tokenizing Text\n",
    "Tokenization là quá trình chuyển đổi văn bản dạng tự do thành danh sách các “tokens” hoặc các từ riêng lẻ. Cách dễ nhất để làm điều này là sử dụng lớp Tokenizer. Transformer này sẽ lấy một chuỗi từ, được phân tách bằng khoảng trắng và chuyển chúng thành một mảng từ. Ví dụ: trong tập dữ liệu của mình, chúng tôi có thể muốn chuyển trường Description thành danh sách các token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS__PAmvlARd",
    "outputId": "06b3f414-6f90-4b73-c3f3-c3101baffcff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, , water, transfer, tattoos]      |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, , doorstop, red]         |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eT6kv9EklARd"
   },
   "source": [
    "Chúng ta cũng có thể tạo một Tokenizer không chỉ dựa trên khoảng trắng mà còn là một biểu thức chính quy với RegexTokenizer. Định dạng của biểu thức chính quy phải tuân theo cú pháp Java Regular Expression (RegEx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ge4tWrrVlARe",
    "outputId": "bd5cbd2a-12f2-49fd-bf47-9bb170ab376e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[rabbit, night, light]                    |\n",
      "|DOUGHNUT LIP GLOSS                 |[doughnut, lip, gloss]                    |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[12, message, cards, with, envelopes]     |\n",
      "|BLUE HARMONICA IN BOX              |[blue, harmonica, in, box]                |\n",
      "|GUMBALL COAT RACK                  |[gumball, coat, rack]                     |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[skulls, water, transfer, tattoos]        |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[feltcraft, girl, amelie, kit]            |\n",
      "|CAMOUFLAGE LED TORCH               |[camouflage, led, torch]                  |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[white, skull, hot, water, bottle]        |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[english, rose, hot, water, bottle]       |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[hot, water, bottle, keep, calm]          |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[scottie, dog, hot, water, bottle]        |\n",
      "|ROSE CARAVAN DOORSTOP              |[rose, caravan, doorstop]                 |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[gingham, heart, doorstop, red]           |\n",
      "|STORAGE TIN VINTAGE LEAF           |[storage, tin, vintage, leaf]             |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[set, of, 4, knick, knack, tins, poppies] |\n",
      "|POPCORN HOLDER                     |[popcorn, holder]                         |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[grow, a, flytrap, or, sunflower, in, tin]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[airline, bag, vintage, world, champion]  |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[airline, bag, vintage, jet, set, brown]  |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"DescOut\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uOAvSA3lARe"
   },
   "source": [
    "Một cách khác để sử dụng RegexTokenizer là sử dụng nó để xuất ra các giá trị khớp với pattern được cung cấp thay vì sử dụng nó như một khoảng trống. Chúng tôi thực hiện điều này bằng cách đặt tham số khoảng trống thành false. Làm điều này với một khoảng trắng dưới dạng một pattern sẽ trả về tất cả các khoảng trắng, điều này không quá hữu ích, nhưng nếu chúng ta đã thực hiện pattern của mình để nắm bắt các từ riêng lẻ, chúng ta có thể trả lại những khoảng trắng đó:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C93By3LzlARe",
    "outputId": "a6d9bca2-e5c0-4ca0-b37d-38cf961cd579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------+\n",
      "|Description                        |DescOut           |\n",
      "+-----------------------------------+------------------+\n",
      "|RABBIT NIGHT LIGHT                 |[ ,  ]            |\n",
      "|DOUGHNUT LIP GLOSS                 |[ ,  ,  ]         |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES    |[ ,  ,  ,  ]      |\n",
      "|BLUE HARMONICA IN BOX              |[ ,  ,  ,  ]      |\n",
      "|GUMBALL COAT RACK                  |[ ,  ]            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS     |[ ,  ,  ,  ,  ]   |\n",
      "|FELTCRAFT GIRL AMELIE KIT          |[ ,  ,  ]         |\n",
      "|CAMOUFLAGE LED TORCH               |[ ,  ]            |\n",
      "|WHITE SKULL HOT WATER BOTTLE       |[ ,  ,  ,  ,  ]   |\n",
      "|ENGLISH ROSE HOT WATER BOTTLE      |[ ,  ,  ,  ]      |\n",
      "|HOT WATER BOTTLE KEEP CALM         |[ ,  ,  ,  ]      |\n",
      "|SCOTTIE DOG HOT WATER BOTTLE       |[ ,  ,  ,  ]      |\n",
      "|ROSE CARAVAN DOORSTOP              |[ ,  ]            |\n",
      "|GINGHAM HEART  DOORSTOP RED        |[ ,  ,  ,  ]      |\n",
      "|STORAGE TIN VINTAGE LEAF           |[ ,  ,  ]         |\n",
      "|SET OF 4 KNICK KNACK TINS POPPIES  |[ ,  ,  ,  ,  ,  ]|\n",
      "|POPCORN HOLDER                     |[ ]               |\n",
      "|GROW A FLYTRAP OR SUNFLOWER IN TIN |[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTAGE WORLD CHAMPION |[ ,  ,  ,  ,  ]   |\n",
      "|AIRLINE BAG VINTAGE JET SET BROWN  |[ ,  ,  ,  ,  ]   |\n",
      "+-----------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    ".setInputCol(\"Description\")\\\n",
    ".setOutputCol(\"DescOut\")\\\n",
    ".setPattern(\" \")\\\n",
    ".setGaps(False)\\\n",
    ".setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUutUr89lARe"
   },
   "source": [
    "## Removing Common Words\n",
    "\n",
    "Một nhiệm vụ phổ biến sau khi mã hóa là lọc các từ dừng, các từ phổ biến không có liên quan trong nhiều loại phân tích và do đó cần được loại bỏ. Các từ dừng thường xuất hiện trong tiếng Anh bao gồm “the,” “and” và “but.” Spark chứa danh sách các từ dừng mặc định mà bạn có thể thấy bằng cách gọi phương thức sau, có thể phân biệt chữ hoa chữ thường nếu cần (kể từ Spark 2.2, các ngôn ngữ được hỗ trợ cho từ dừng là “danish,” “dutch,” “english,” “finnish,” “french,”\n",
    "“german,” “hungarian,” “italian,” “norwegian,” “portuguese,” “russian,” “spanish,” “swedish,”\n",
    "và “turkish”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8cCyybXlARe",
    "outputId": "c95d7b96-067e-4cca-80d4-2483dd47edee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------------------+\n",
      "|         Description|             DescOut|StopWordsRemover_8cda260ef3e0__output|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|                 [rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|                 [doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|                 [12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|                 [blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|                 [gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|                 [skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|                 [feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|                 [camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|                 [white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|                 [english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|                 [hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|                 [scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|                 [rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|                 [gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|                 [storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|                 [set, 4, knick, k...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|                    [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|                 [grow, flytrap, s...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|                 [airline, bag, vi...|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    ".setStopWords(englishStopWords)\\\n",
    ".setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIfysFTalARf"
   },
   "source": [
    "Lưu ý cách từ of được xóa trong cột đầu ra. Đó là bởi vì nó là một từ phổ biến đến mức không liên quan đến bất kỳ thao tác downstream nào và chỉ thêm nhiễu vào tập dữ liệu của chúng tôi.\n",
    "\n",
    "## Creating Word Combinations\n",
    "Việc tokenize các chuỗi và lọc các từ dừng giúp chúng tôi có một tập hợp các từ sạch để sử dụng làm feature. Người ta thường quan tâm đến sự kết hợp của các từ, thường là bằng cách xem các từ được sắp xếp theo vị trí. Các tổ hợp từ về mặt kỹ thuật được gọi là n-gram — nghĩa là, chuỗi các từ có độ dài n. Một n-gam chiều dài 1 được gọi là đơn vị; những thứ có độ dài 2 được gọi là bigram, và độ dài 3 được gọi là trigram (bất cứ thứ gì ở trên chúng chỉ là 4-gram, 5-gram, v.v.), Thứ tự quan trọng với việc tạo n gam, vì vậy chuyển một câu có ba từ thành biểu diễn bigram sẽ dẫn đến hai bigram. Mục tiêu khi tạo n-gram là để nắm bắt tốt hơn cấu trúc câu và nhiều thông tin hơn những gì có thể thu thập được bằng cách chỉ nhìn vào tất cả các từ riêng lẻ. Hãy tạo một số n-gram để minh họa khái niệm này\n",
    "\n",
    "Với n-gram, chúng ta có thể xem xét chuỗi các từ thường cùng xuất hiện và sử dụng chúng làm đầu vào cho một thuật toán học máy. Những điều này có thể tạo ra các tính năng tốt hơn so với việc chỉ nhìn vào tất cả các từ riêng lẻ (giả sử được tokenize trên một ký tự khoảng trắng):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb9KcjhUlARf",
    "outputId": "39fc1d9c-699f-459d-8249-200cc168cb24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------------+\n",
      "|DescOut               |NGram_37abaeb63aca__output|\n",
      "+----------------------+--------------------------+\n",
      "|[rabbit, night, light]|[rabbit, night, light]    |\n",
      "|[doughnut, lip, gloss]|[doughnut, lip, gloss]    |\n",
      "+----------------------+--------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------------+---------------------------+\n",
      "|DescOut               |NGram_254deca9717c__output |\n",
      "+----------------------+---------------------------+\n",
      "|[rabbit, night, light]|[rabbit night, night light]|\n",
      "|[doughnut, lip, gloss]|[doughnut lip, lip gloss]  |\n",
      "+----------------------+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(2, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2YYNNdWlARf"
   },
   "source": [
    "## Converting Words into Numerical Representations\n",
    "\n",
    "Sau khi bạn có các feature của từ, đã đến lúc bắt đầu đếm các số lượng từ và kết hợp từ để sử dụng trong các mô hình của chúng tôi. Cách đơn giản nhất là chỉ cần bao gồm số lượng nhị phân của một từ trong một tài liệu nhất định (trong trường hợp của chúng tôi là một hàng). Về cơ bản, chúng tôi đang đo lường xem mỗi hàng có chứa một từ nhất định hay không. Đây là một cách đơn giản để chuẩn hóa kích thước tài liệu và số lần xuất hiện cũng như nhận các feature số cho phép chúng tôi phân loại tài liệu dựa trên nội dung. Ngoài ra, chúng ta có thể đếm các từ bằng cách sử dụng CountVectorizer hoặc cân lại chúng theo mức độ phổ biến của một từ nhất định trong tất cả các tài liệu bằng cách sử dụng phép biến đổi TF-IDF (thảo luận tiếp theo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAYmGxw4lARf",
    "outputId": "57c287f6-a605-4af3-a07e-3d65eff44d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|Description                    |DescOut                              |countVec                                     |\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |(500,[150,185,212],[1.0,1.0,1.0])            |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |(500,[462,463,492],[1.0,1.0,1.0])            |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|(500,[35,41,166],[1.0,1.0,1.0])              |\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |(500,[10,16,36,352],[1.0,1.0,1.0,1.0])       |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |(500,[228,280,407],[1.0,1.0,1.0])            |\n",
      "|SKULLS  WATER TRANSFER TATTOOS |[skulls, , water, transfer, tattoos] |(500,[11,40,133],[1.0,1.0,1.0])              |\n",
      "|FELTCRAFT GIRL AMELIE KIT      |[feltcraft, girl, amelie, kit]       |(500,[60,64,69],[1.0,1.0,1.0])               |\n",
      "|CAMOUFLAGE LED TORCH           |[camouflage, led, torch]             |(500,[263],[1.0])                            |\n",
      "|WHITE SKULL HOT WATER BOTTLE   |[white, skull, hot, water, bottle]   |(500,[15,34,39,40,118],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|ENGLISH ROSE HOT WATER BOTTLE  |[english, rose, hot, water, bottle]  |(500,[34,39,40,46,169],[1.0,1.0,1.0,1.0,1.0])|\n",
      "+-------------------------------+-------------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    ".setInputCol(\"DescOut\")\\\n",
    ".setOutputCol(\"countVec\")\\\n",
    ".setVocabSize(500)\\\n",
    ".setMinTF(1)\\\n",
    ".setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43XgVZ55lARf"
   },
   "source": [
    "\n",
    "Mặc dù kết quả đầu ra trông hơi phức tạp, nhưng nó thực sự chỉ là một vectơ thưa chứa tổng kích thước volcabulary, chỉ mục của từ trong vocalbulary và sau đó là số lượng của từ cụ thể đó:\n",
    "\n",
    "## Term frequency–inverse document frequency\n",
    "\n",
    "Một cách khác để tiếp cận vấn đề chuyển đổi văn bản thành biểu diễn số là sử dụng thuật ngữ tần số- nghịch đảo tần số (TF – IDF). Nói một cách đơn giản nhất, TF-IDF đo lường tần suất một từ xuất hiện trong mỗi tài liệu, được tính theo số lượng tài liệu mà từ đó xuất hiện trong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFZMXTVWlARf",
    "outputId": "2b011d87-68e6-45cf-a5ef-36ab099aec23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[large, red, babushka, notebook]       |\n",
      "|[red, retrospot, oven, glove]          |\n",
      "|[red, retrospot, plate]                |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn = tokenized\\\n",
    ".where(\"array_contains(DescOut, 'red')\")\\\n",
    ".select(\"DescOut\")\\\n",
    ".limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQY05GRplARf"
   },
   "source": [
    "Chúng ta có thể thấy một số từ trùng lặp trong các tài liệu này, nhưng những từ này ít nhất cung cấp một biểu diễn giống-chủ đề thô. Bây giờ hãy nhập nó vào TF – IDF. Để làm điều này, chúng tôi sẽ băm từng từ và chuyển nó thành một biểu diễn số, sau đó cân nhắc từng từ trong ngôn ngữ theo tần suất nghịch đảo của tài liệu. Hashing là một quá trình tương tự như CountVectorizer, nhưng không thể thay đổi được — nghĩa là, từ chỉ mục đầu ra của chúng tôi cho một từ, chúng tôi không thể lấy từ đầu vào của mình (nhiều từ có thể ánh xạ tới cùng một chỉ mục đầu ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rOd8U9QlARg"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    ".setInputCol(\"DescOut\")\\\n",
    ".setOutputCol(\"TFOut\")\\\n",
    ".setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    ".setInputCol(\"TFOut\")\\\n",
    ".setOutputCol(\"IDFOut\")\\\n",
    ".setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOuiONjIlARg",
    "outputId": "ad9fe650-21de-4bff-906e-ee0d84c1109b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                |IDFOut                                                                                                           |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[50,52,415,6756,8005],[1.0,1.0,1.0,1.0,1.0])  |(10000,[50,52,415,6756,8005],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[52,4995,8737,9001],[1.0,1.0,1.0,1.0])        |(10000,[52,4995,8737,9001],[0.0,0.0,0.0,0.0])                                                                    |\n",
      "|[pin, cushion, babushka, red]          |(10000,[52,610,2490,7153],[1.0,1.0,1.0,1.0])         |(10000,[52,610,2490,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[52,547,6703,8448],[1.0,1.0,1.0,1.0])         |(10000,[52,547,6703,8448],[0.0,0.0,0.0,1.0116009116784799])                                                      |\n",
      "|[red, kitchen, scales]                 |(10000,[52,756,6452],[1.0,1.0,1.0])                  |(10000,[52,756,6452],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[52,804,3372,6594,9808],[1.0,1.0,1.0,1.0,1.0])|(10000,[52,804,3372,6594,9808],[0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[52,2787,7022,7153],[1.0,1.0,1.0,1.0])        |(10000,[52,2787,7022,7153],[0.0,0.0,0.0,1.2992829841302609])                                                     |\n",
      "|[red, retrospot, oven, glove]          |(10000,[52,8242,8448,8667],[1.0,1.0,1.0,1.0])        |(10000,[52,8242,8448,8667],[0.0,0.0,1.0116009116784799,0.0])                                                     |\n",
      "|[red, retrospot, plate]                |(10000,[52,4925,8448],[1.0,1.0,1.0])                 |(10000,[52,4925,8448],[0.0,0.0,1.0116009116784799])                                                              |\n",
      "+---------------------------------------+-----------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvrFhH6ZlARg"
   },
   "source": [
    "(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])\n",
    "\n",
    "Vectơ này được biểu diễn bằng ba giá trị khác nhau: tổng kích thước volcabulary, hàm băm của mỗi từ xuất hiện trong tài liệu và trọng số của từng thuật ngữ đó. Điều này tương tự với đầu ra CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CghU_1xOlARg"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec là một công cụ dựa trên học sâu để tính toán biểu diễn vectơ của một tập hợp các từ. Mục đích là có các từ tương tự gần nhau trong không gian vectơ này, vì vậy chúng ta có thể khái quát hóa về các từ đó. Mô hình này dễ đào tạo và sử dụng, đồng thời đã được chứng minh là hữu ích trong một số ứng dụng xử lý ngôn ngữ tự nhiên, bao gồm nhận dạng thực thể, định dạng, phân tích cú pháp, gắn thẻ và dịch máy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3A6Un48RlARg",
    "outputId": "a8551a71-7915-4de6-b1b7-1651a12eeb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.02036476763896644,0.00627172663807869,0.0049649406224489216]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.06342963767903191,0.01215339491942099,-0.05633769024695669]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.017420613393187522,0.008711700141429902,-0.013818052783608438]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "(\"Hi I heard about Spark\".split(\" \"), ),\n",
    "(\"I wish Java could use case classes\".split(\" \"), ),\n",
    "(\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6IAKcprlARg"
   },
   "source": [
    "# Feature Manipulation\n",
    "\n",
    "Mặc dù gần như mọi transformer trong ML thao tác không gian feature theo một cách nào đó, các thuật toán và công cụ sau đây là các phương tiện tự động để mở rộng các vectơ feature đầu vào hoặc giảm chúng xuống một số kích thước thấp hơn.\n",
    "\n",
    "## PCA\n",
    "\n",
    "Principal Components Analysis (PCA) là một kỹ thuật toán học để tìm ra các khía cạnh quan trọng nhất của dữ liệu của chúng ta (các thành phần chính). Nó thay đổi cách biểu diễn các feature của dữ liệu của chúng tôi bằng cách tạo một tập hợp các fearture mới (“các khía cạnh”). Mỗi feature mới là sự kết hợp của các feature ban đầu. Sức mạnh của PCA là nó có thể tạo ra một tập hợp nhỏ các feature có ý nghĩa hơn để đưa vào mô hình của bạn, với chi phí tiềm năng là khả năng diễn giải\n",
    "\n",
    "Bạn nên sử dụng PCA nếu bạn có tập dữ liệu đầu vào lớn và muốn giảm tổng số feature bạn có. Điều này thường xuất hiện trong phân tích văn bản trong đó toàn bộ không gian feature là rất lớn và nhiều feature phần lớn không liên quan. Sử dụng PCA, chúng tôi có thể tìm thấy các tổ hợp feature quan trọng nhất và chỉ đưa những feature đó vào mô hình học máy của chúng tôi. PCA nhận một tham số 􁽅, chỉ định số lượng các feature đầu ra cần tạo. Nói chung, kích thước này phải nhỏ hơn nhiều so với kích thước của vectơ đầu vào của bạn.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "Chọn đúng 􁽅 không phải là chuyện nhỏ và không có phương án nào mà chúng tôi có thể đưa ra. Xem các chương có liên quan trong ESL và ISL để biết thêm thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS1fyjJClARg",
    "outputId": "26f62815-97b7-41a1-a3c5-d9303a5aa24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_7e5eb8219922__output                  |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.0713719499248417,-0.4526654888147805]  |\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073723,1.2593401322219198]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060155975]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY2XVAEqlARg"
   },
   "source": [
    "## Interaction\n",
    "\n",
    "Trong một số trường hợp, bạn có thể có kiến thức miền về các biến cụ thể trong tập dữ liệu của mình. Ví dụ: bạn có thể biết rằng một sự tương tác nhất định giữa hai biến là một biến quan trọng để đưa vào downstream estimator. feature transformer Interaction cho phép bạn tạo tương tác giữa hai biến theo cách thủ công. Nó chỉ nhân hai đối tượng với nhau — điều mà một mô hình tuyến tính điển hình sẽ không làm được cho mọi cặp đối tượng có thể có trong dữ liệu của bạn. Máy biến áp này hiện chỉ có sẵn trực tiếp trong Scala nhưng có thể được gọi từ bất kỳ ngôn ngữ nào bằng RFormula. Chúng tôi khuyến nghị người dùng chỉ sử dụng RFormula thay vì tạo tương tác theo cách thủ công.\n",
    "## Polynomial Expansion\n",
    "\n",
    "Khai triển đa thức được sử dụng để tạo các biến tương tác của tất cả các cột đầu vào. Với khai triển đa thức, chúng tôi chỉ định mức độ chúng tôi muốn thấy các tương tác khác nhau. Ví dụ: đối với đa thức bậc 2, Spark lấy mọi giá trị trong feature vector của chúng tôi, nhân nó với mọi giá trị khác trong feature vector và sau đó lưu trữ kết quả dưới dạng feature. Ví dụ: nếu chúng ta có hai feature đầu vào, chúng ta sẽ nhận được bốn feature đầu ra nếu chúng ta sử dụng đa thức bậc hai (2x2). Nếu chúng ta có ba feature đầu vào, chúng ta sẽ nhận được chín feature đầu ra (3x3). Nếu chúng ta sử dụng đa thức bậc ba, chúng ta sẽ nhận được 27 feature đầu ra (3x3x3), v.v. Sự chuyển đổi này hữu ích khi bạn muốn xem các tương tác giữa các feature cụ thể nhưng không nhất thiết phải chắc chắn về những tương tác nào cần xem xét.\n",
    "\n",
    "### WARNING\n",
    "\n",
    "Việc khai triển đa thức có thể làm tăng đáng kể không gian feature của bạn, dẫn đến cả chi phí tính toán cao và overfitting. Sử dụng nó một cách thận trọng, đặc biệt là cho các mức độ cao hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKGjnhlclARg",
    "outputId": "57abf229-8223-4b51-ebe2-e998e1de1fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+----------------------------------------+\n",
      "| id|      features|PolynomialExpansion_380dbe0ba69d__output|\n",
      "+---+--------------+----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  0|[1.0,0.1,-1.0]|                    [1.0,1.0,0.1,0.1,...|\n",
      "|  1| [2.0,1.1,1.0]|                    [2.0,4.0,1.1,2.2,...|\n",
      "|  1|[3.0,10.1,3.0]|                    [3.0,9.0,10.1,30....|\n",
      "+---+--------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "pe = PolynomialExpansion().setInputCol(\"features\").setDegree(2)\n",
    "pe.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9RciM_JlARh"
   },
   "source": [
    "# Feature Selection\n",
    "Thông thường, bạn sẽ có một loạt các feature có thể có và muốn chọn một tập hợp con nhỏ hơn để sử dụng cho việc training. Ví dụ: nhiều feature có thể tương quan hoặc sử dụng quá nhiều feature có thể dẫn đến overfitting. Quá trình này được gọi là lựa chọn feature. Có một số cách để đánh giá tầm quan trọng của feature sau khi bạn đã train một mô hình nhưng một tùy chọn khác là thực hiện một số lọc thô trước. Spark có một số tùy chọn đơn giản để làm điều đó, chẳng hạn như ChiSqSelector.\n",
    "## ChiSqSelector\n",
    "\n",
    "ChiSqSelector tận dụng một test thống kê để xác định các feature không độc lập với nhãn mà chúng tôi đang cố gắng dự đoán và loại bỏ các feature không liên quan. Nó thường được sử dụng với dữ liệu phân loại để giảm số lượng các feature bạn sẽ nhập vào mô hình của mình, cũng như giảm kích thước của dữ liệu văn bản (ở dạng tần số hoặc số lượng). Vì phương pháp này dựa trên kiểm tra Chi-Square, nên có một số cách khác nhau để chúng tôi có thể chọn các tính năng \"tốt nhất\". Các phương thức là numTopFeatures, được sắp xếp theo giá trị p-value; phân vị, chiếm một tỷ lệ của các feature đầu vào (thay vì chỉ N feature tốt nhất); và fpr, đặt giá trị p-value bị cắt.\n",
    "\n",
    "Chúng tôi sẽ chứng minh điều này với đầu ra của CountVectorizer được tạo trước đó trong chương này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8haU6aulARh",
    "outputId": "69072d9b-eb60-4b3e-e12c-d49e6f0b9f68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 59210)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o971.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e76f5dfc2b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msetNumTopFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mchisq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprechi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprechi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customerId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Description\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DescOut\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o971.fit"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector, Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn\\\n",
    ".transform(sales.select(\"Description\", \"CustomerId\"))\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "prechi = fittedCV.transform(tokenized)\\\n",
    ".where(\"CustomerId IS NOT NULL\")\n",
    "chisq = ChiSqSelector()\\\n",
    ".setFeaturesCol(\"countVec\")\\\n",
    ".setLabelCol(\"CustomerId\")\\\n",
    ".setNumTopFeatures(2)\n",
    "\n",
    "chisq.fit(prechi).transform(prechi)\\\n",
    ".drop(\"customerId\", \"Description\", \"DescOut\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMXkZjQMlARh"
   },
   "source": [
    "# Advanced Topics\n",
    "Có một số chủ đề nâng cao xoay quanh transformers và estimators. Ở đây chúng tôi đề cập đến hai transformer phổ biến nhất, bền bỉ nhất cũng như viết các transformer tùy chỉnh.\n",
    "## Persisting Transformers\n",
    "Khi bạn đã sử dụng estimator để định cấu hình transformer, có thể hữu ích khi ghi nó vào đĩa và chỉ cần tải nó khi cần thiết (ví dụ: để sử dụng trong một phiên Spark khác). Chúng tôi đã thấy điều này trong chương trước khi chúng tôi duy trì toàn bộ pipeline. Để duy trì một transformer riêng lẻ, chúng tôi sử dụng phương pháp ghi trên transformer được chọn (fitted) (hoặc transformer tiêu chuẩn) và chỉ định vị trí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0LZXYKslARh"
   },
   "outputs": [],
   "source": [
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"/tmp/fittedPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbBqYgfblARh"
   },
   "source": [
    "\n",
    "Sau đó, chúng tôi có thể load lại nó trong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbYtVw2qlARh"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"/tmp/fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce09Oo6RlARh"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Chương này đã giới thiệu cơ bản về nhiều phép biến đổi tiền xử lý phổ biến nhất mà Spark có sẵn. Có một số miền cụ thể mà chúng tôi không có đủ chỗ để đề cập (ví dụ: Biến đổi Cosin rời rạc), nhưng bạn có thể tìm thêm thông tin trong tài liệu. Lĩnh vực này của Spark cũng không ngừng phát triển khi cộng đồng phát triển những cái mới.\n",
    "\n",
    "Một khía cạnh quan trọng khác của bộ công cụ feature engineering này là tính nhất quán. Trong chương trước, chúng ta đã đề cập đến khái niệm đường ống pipeline, một công cụ thiết yếu để đóng gói và train quy trình làm việc ML từ đầu đến cuối. Trong chương tiếp theo, chúng ta sẽ bắt đầu xem xét nhiều loại tác vụ học máy mà bạn có thể có và những thuật toán nào có sẵn cho mỗi tác vụ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTxDPtnclARh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Chapter 25. Preprocessing and Feature Engineering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
